{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOBzr8cYp1aJ8deupARx8ck",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kognavi/sentiment-analysis-test/blob/main/sentiment_analysis_test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d4zoZ8zn7xOy",
        "outputId": "dad5bec1-97d5-4862-dc50-d9723d30f2c1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ERROR: unknown command \"in!pip\"\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.5.1)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.18.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.2.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (5.29.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.13.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.71.0)\n",
            "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.18.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.8.0)\n",
            "Collecting numpy>=1.17 (from transformers)\n",
            "  Downloading numpy-2.0.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.9/60.9 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.13.0)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.20.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.0.9)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.15.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.8)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.19.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n",
            "Downloading numpy-2.0.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (19.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.5/19.5 MB\u001b[0m \u001b[31m28.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.2.5\n",
            "    Uninstalling numpy-2.2.5:\n",
            "      Successfully uninstalled numpy-2.2.5\n",
            "Successfully installed numpy-2.0.2\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.6.0)\n",
            "Requirement already satisfied: numpy<3.0.0,>=1.17 in /usr/local/lib/python3.11/dist-packages (from accelerate) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (24.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from accelerate) (6.0.2)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (2.6.0+cu124)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (0.30.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from accelerate) (0.5.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate) (2025.3.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.0->accelerate) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2025.1.31)\n"
          ]
        }
      ],
      "source": [
        "!pip in!pip install transformers datasets tensorflow # TensorFlowを使う場合\n",
        "!pip install transformers datasets tensorflow\n",
        "# !pip install transformers datasets torch # PyTorchを使う場合 (今回はTensorFlowで進めます)\n",
        "!pip install accelerate -U # Trainerを使う場合に推奨\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade --force-reinstall scipy\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hD4CpLmBubev",
        "outputId": "84878494-58ce-401b-b584-9ba78a8f9e47"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting scipy\n",
            "  Using cached scipy-1.15.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "Collecting numpy<2.5,>=1.23.5 (from scipy)\n",
            "  Using cached numpy-2.2.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
            "Using cached scipy-1.15.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (37.6 MB)\n",
            "Using cached numpy-2.2.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.4 MB)\n",
            "Installing collected packages: numpy, scipy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.15.2\n",
            "    Uninstalling scipy-1.15.2:\n",
            "      Successfully uninstalled scipy-1.15.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 2.2.5 which is incompatible.\n",
            "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.2.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-2.2.5 scipy-1.15.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ステップ2: データセットの準備\n",
        "from datasets import load_dataset\n",
        "\n",
        "# IMDbデータセットをロード\n",
        "imdb_dataset = load_dataset(\"imdb\")\n",
        "\n",
        "# データセットの構造を確認\n",
        "print(imdb_dataset)\n",
        "\n",
        "# 訓練データの一部を確認\n",
        "print(\"\\n訓練データの例:\")\n",
        "print(imdb_dataset[\"train\"][0])\n",
        "\n",
        "# 訓練用と検証用に分割\n",
        "train_dataset = imdb_dataset[\"train\"]\n",
        "val_dataset = imdb_dataset[\"test\"]\n",
        "\n",
        "# (オプション) データ量を減らす場合\n",
        "# train_dataset = imdb_dataset[\"train\"].shuffle(seed=42).select(range(1000))\n",
        "# val_dataset = imdb_dataset[\"test\"].shuffle(seed=42).select(range(1000))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XCO1l3gM_iV1",
        "outputId": "79670b82-9acf-4ad9-98db-43ffe98c299a"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['text', 'label'],\n",
            "        num_rows: 25000\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['text', 'label'],\n",
            "        num_rows: 25000\n",
            "    })\n",
            "    unsupervised: Dataset({\n",
            "        features: ['text', 'label'],\n",
            "        num_rows: 50000\n",
            "    })\n",
            "})\n",
            "\n",
            "訓練データの例:\n",
            "{'text': 'I rented I AM CURIOUS-YELLOW from my video store because of all the controversy that surrounded it when it was first released in 1967. I also heard that at first it was seized by U.S. customs if it ever tried to enter this country, therefore being a fan of films considered \"controversial\" I really had to see this for myself.<br /><br />The plot is centered around a young Swedish drama student named Lena who wants to learn everything she can about life. In particular she wants to focus her attentions to making some sort of documentary on what the average Swede thought about certain political issues such as the Vietnam War and race issues in the United States. In between asking politicians and ordinary denizens of Stockholm about their opinions on politics, she has sex with her drama teacher, classmates, and married men.<br /><br />What kills me about I AM CURIOUS-YELLOW is that 40 years ago, this was considered pornographic. Really, the sex and nudity scenes are few and far between, even then it\\'s not shot like some cheaply made porno. While my countrymen mind find it shocking, in reality sex and nudity are a major staple in Swedish cinema. Even Ingmar Bergman, arguably their answer to good old boy John Ford, had sex scenes in his films.<br /><br />I do commend the filmmakers for the fact that any sex shown in the film is shown for artistic purposes rather than just to shock people and make money to be shown in pornographic theaters in America. I AM CURIOUS-YELLOW is a good film for anyone wanting to study the meat and potatoes (no pun intended) of Swedish cinema. But really, this film doesn\\'t have much of a plot.', 'label': 0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ステップ3: データセットの準備 (raw_datasets -> imdb_dataset 修正版)\n",
        "from transformers import AutoTokenizer\n",
        "import tensorflow as tf\n",
        "\n",
        "# トークナイザのロード\n",
        "model_checkpoint = \"distilbert-base-uncased\"\n",
        "try:\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
        "    print(\"トークナイザのロード完了。\")\n",
        "except Exception as e:\n",
        "    print(f\"トークナイザのロード中にエラーが発生しました: {e}\")\n",
        "    # エラー処理をここに記述することも可能です (例: プログラム終了)\n",
        "\n",
        "# トークン化関数\n",
        "def tokenize_function(examples):\n",
        "    # \"text\" カラムが存在するか確認してからアクセス\n",
        "    if \"text\" in examples:\n",
        "        return tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\", max_length=512)\n",
        "    else:\n",
        "        # \"text\" カラムがない場合の処理 (エラーを出すか、空の辞書を返すなど)\n",
        "        print(\"警告: 'text' カラムが見つかりません。空のトークン化結果を返します。\")\n",
        "        # 必要に応じてエラー処理を追加\n",
        "        return {} # または適切なデフォルト値を返す\n",
        "\n",
        "# データセットのトークン化\n",
        "try:\n",
        "    print(\"トークン化を開始します... (少し時間がかかる場合があります)\")\n",
        "    # === 修正箇所 ===\n",
        "    # raw_datasets を imdb_dataset に変更\n",
        "    tokenized_datasets = imdb_dataset.map(tokenize_function, batched=True)\n",
        "    # ================\n",
        "    print(\"トークン化が完了しました。\")\n",
        "except NameError as e:\n",
        "     print(f\"トークン化中に NameError が発生しました: {e}\")\n",
        "     print(\"-> 'imdb_dataset' 変数が正しくロードされているか確認してください。\")\n",
        "except KeyError as e:\n",
        "     print(f\"トークン化中に KeyError が発生しました: {e}\")\n",
        "     print(\"-> データセットに 'text' カラムが存在するか確認してください。\")\n",
        "except Exception as e:\n",
        "    print(f\"トークン化中に予期せぬエラーが発生しました: {e}\")\n",
        "    # その他のエラー処理\n",
        "\n",
        "# 'tokenized_datasets' が正しく生成されたか確認してから後続処理へ\n",
        "if 'tokenized_datasets' in locals():\n",
        "\n",
        "    # 不要なカラムの削除 (より安全な方法)\n",
        "    if 'text' in tokenized_datasets['train'].column_names:\n",
        "        tokenized_datasets = tokenized_datasets.remove_columns([\"text\"])\n",
        "        print(\"'text' カラムを削除しました。\")\n",
        "    else:\n",
        "        print(\"'text' カラムは既に削除されているか、存在しません。\")\n",
        "\n",
        "    # ラベルカラムの名前を変更 (TensorFlowモデルが期待する 'labels' に)\n",
        "    if 'label' in tokenized_datasets['train'].column_names:\n",
        "        tokenized_datasets = tokenized_datasets.rename_column(\"label\", \"labels\")\n",
        "        print(\"'label' カラムを 'labels' に変更しました。\")\n",
        "    elif 'labels' in tokenized_datasets['train'].column_names:\n",
        "         print(\"'labels' カラムは既に存在します。\")\n",
        "    else:\n",
        "        print(\"警告: 'label' または 'labels' カラムが見つかりません。\")\n",
        "\n",
        "\n",
        "    # データセットをTensorFlow形式に変換\n",
        "    batch_size = 16 # バッチサイズ (GPUメモリに応じて調整)\n",
        "\n",
        "    try:\n",
        "        print(\"\\nモデルの prepare_tf_dataset を使用して TF データセットを準備します...\")\n",
        "        tf_train_dataset = model.prepare_tf_dataset(\n",
        "            tokenized_datasets[\"train\"],\n",
        "            shuffle=True,\n",
        "            batch_size=batch_size,\n",
        "            tokenizer=tokenizer\n",
        "        )\n",
        "\n",
        "        tf_val_dataset = model.prepare_tf_dataset(\n",
        "            tokenized_datasets[\"test\"],\n",
        "            shuffle=False,\n",
        "            batch_size=batch_size,\n",
        "            tokenizer=tokenizer\n",
        "        )\n",
        "        print(\"データセットの準備が完了しました。\")\n",
        "\n",
        "        # データセット形式の確認\n",
        "        print(\"\\nトークン化・バッチ化後の訓練データセットの形式:\")\n",
        "        print(tf_train_dataset.element_spec)\n",
        "\n",
        "        print(\"\\nトークン化・バッチ化後の検証データセットの形式:\")\n",
        "        print(tf_val_dataset.element_spec)\n",
        "\n",
        "        # 最初の訓練バッチの例を確認\n",
        "        print(\"\\n最初の訓練バッチの例:\")\n",
        "        for batch in tf_train_dataset.take(1):\n",
        "            if isinstance(batch, tuple) and len(batch) == 2:\n",
        "                features, labels = batch\n",
        "                print(\"バッチは (features, labels) のタプル形式です。\")\n",
        "                if isinstance(features, dict):\n",
        "                     print(\"  Features (辞書) のキー:\", features.keys())\n",
        "                     print(\"  Features['input_ids'] の形状:\", features['input_ids'].shape)\n",
        "                     print(\"  Features['attention_mask'] の形状:\", features['attention_mask'].shape)\n",
        "                else:\n",
        "                     print(\"  Features の型:\", type(features))\n",
        "                print(\"  Labels の形状:\", labels.shape)\n",
        "            elif isinstance(batch, dict):\n",
        "                print(\"バッチは辞書形式です。\")\n",
        "                print(\"  キー:\", batch.keys())\n",
        "                # ... (以前のコードと同様)\n",
        "            else:\n",
        "                print(f\"予期しないバッチ形式です: {type(batch)}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\nTFデータセットの準備中にエラーが発生しました: {e}\")\n",
        "        # エラー処理\n",
        "\n",
        "else:\n",
        "    print(\"エラー: 'tokenized_datasets' が定義されなかったため、処理を続行できません。\")\n",
        "    print(\"-> トークン化のステップでエラーが発生していないか確認してください。\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wcf5ZwC4cStk",
        "outputId": "cb18a7f6-da44-4b29-da2a-76e8138b60e8"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "トークナイザのロード完了。\n",
            "トークン化を開始します... (少し時間がかかる場合があります)\n",
            "トークン化が完了しました。\n",
            "'text' カラムを削除しました。\n",
            "'label' カラムを 'labels' に変更しました。\n",
            "\n",
            "モデルの prepare_tf_dataset を使用して TF データセットを準備します...\n",
            "データセットの準備が完了しました。\n",
            "\n",
            "トークン化・バッチ化後の訓練データセットの形式:\n",
            "({'input_ids': TensorSpec(shape=(16, 512), dtype=tf.int64, name=None), 'attention_mask': TensorSpec(shape=(16, 512), dtype=tf.int64, name=None)}, TensorSpec(shape=(16,), dtype=tf.int64, name=None))\n",
            "\n",
            "トークン化・バッチ化後の検証データセットの形式:\n",
            "({'input_ids': TensorSpec(shape=(None, 512), dtype=tf.int64, name=None), 'attention_mask': TensorSpec(shape=(None, 512), dtype=tf.int64, name=None)}, TensorSpec(shape=(None,), dtype=tf.int64, name=None))\n",
            "\n",
            "最初の訓練バッチの例:\n",
            "バッチは (features, labels) のタプル形式です。\n",
            "  Features (辞書) のキー: dict_keys(['input_ids', 'attention_mask'])\n",
            "  Features['input_ids'] の形状: (16, 512)\n",
            "  Features['attention_mask'] の形状: (16, 512)\n",
            "  Labels の形状: (16,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ステップ4: モデルの準備\n",
        "from transformers import TFAutoModelForSequenceClassification\n",
        "import tensorflow as tf\n",
        "\n",
        "# 事前学習済みモデルをシーケンス分類タスク用にロード\n",
        "# num_labels=2: ポジティブ(1)とネガティブ(0)の2クラス分類\n",
        "try:\n",
        "    model = TFAutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=2)\n",
        "    print(\"モデルのロードが完了しました。\")\n",
        "except Exception as e:\n",
        "    print(f\"モデルのロード中にエラーが発生しました: {e}\")\n",
        "    print(\"GPUメモリが不足している可能性があります。ランタイムを再起動し、バッチサイズを小さくして試してください。\")\n",
        "    # 必要であればここで処理を中断\n",
        "\n",
        "# モデルの構造を確認 (オプション)\n",
        "# model.summary() # Kerasモデルとして構造を表示\n",
        "\n",
        "\n",
        "# 'tokenized_datasets' が正しく生成されたか確認してから後続処理へ\n",
        "# (ステップ3で定義されている必要があります)\n",
        "if 'tokenized_datasets' in locals():\n",
        "\n",
        "    # データセットをTensorFlow形式に変換\n",
        "    batch_size = 16 # バッチサイズ (GPUメモリに応じて調整)\n",
        "\n",
        "    try:\n",
        "        print(\"\\nモデルの prepare_tf_dataset を使用して TF データセットを準備します...\")\n",
        "        tf_train_dataset = model.prepare_tf_dataset(\n",
        "            tokenized_datasets[\"train\"],\n",
        "            shuffle=True,\n",
        "            batch_size=batch_size,\n",
        "            tokenizer=tokenizer\n",
        "        )\n",
        "\n",
        "        tf_val_dataset = model.prepare_tf_dataset(\n",
        "            tokenized_datasets[\"test\"],\n",
        "            shuffle=False,\n",
        "            batch_size=batch_size,\n",
        "            tokenizer=tokenizer\n",
        "        )\n",
        "        print(\"データセットの準備が完了しました。\")\n",
        "    except Exception as e:\n",
        "        print(f\"\\nTFデータセットの準備中にエラーが発生しました: {e}\")\n",
        "        # エラー処理\n",
        "        # ここで適切なエラー処理を追加 (例: プログラム終了)\n",
        "        raise  # エラーを再送出して処理を中断\n",
        "\n",
        "    # オプティマイザ、損失関数、メトリクスを設定してモデルをコンパイル\n",
        "    # オプティマイザ: AdamW (Adam Weight Decay) が推奨されることが多い\n",
        "    # 学習率のスケジューリングを使うと、より安定した学習が期待できる\n",
        "    num_train_steps = len(tf_train_dataset) # 1エポックあたりのステップ数 (tf_train_dataset が定義された後に計算)\n",
        "    num_epochs = 3 # エポック数 (調整可能)\n",
        "\n",
        "    # 学習率スケジューラ (例: 線形減衰)\n",
        "    lr_scheduler = tf.keras.optimizers.schedules.PolynomialDecay(\n",
        "        initial_learning_rate=5e-5, # 初期学習率 (推奨値の例)\n",
        "        decay_steps=num_train_steps * num_epochs,\n",
        "        end_learning_rate=0.0,\n",
        "    )\n",
        "    optimizer = tf.keras.optimizers.AdamW(learning_rate=lr_scheduler)\n",
        "\n",
        "    # 損失関数: スパースカテゴリカルクロスエントロピー (ラベルが整数のため)\n",
        "    loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "    # from_logits=True: モデルの出力がロジット (確率に変換される前の値) であることを示す\n",
        "\n",
        "    # メトリクス: 正確度 (Accuracy)\n",
        "    metrics = ['accuracy']\n",
        "\n",
        "    # モデルのコンパイル\n",
        "    model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n",
        "\n",
        "    print(\"\\nモデルのコンパイルが完了しました。\")\n",
        "    print(f\"オプティマイザ: {type(optimizer).__name__}\")\n",
        "    print(f\"損失関数: {type(loss).__name__}\")\n",
        "    print(f\"メトリクス: {metrics}\")\n",
        "else:\n",
        "    print(\"エラー: 'tokenized_datasets' が定義されなかったため、モデルの準備を続行できません。\")\n",
        "    print(\"-> トークン化のステップでエラーが発生していないか確認してください。\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OqXtW4coO0Z9",
        "outputId": "e0b6064c-59f5-48ae-ae1c-2c2999b7a373"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFDistilBertForSequenceClassification: ['vocab_transform.bias', 'vocab_projector.bias', 'vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias']\n",
            "- This IS expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights or buffers of the TF 2.0 model TFDistilBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "モデルのロードが完了しました。\n",
            "\n",
            "モデルの prepare_tf_dataset を使用して TF データセットを準備します...\n",
            "データセットの準備が完了しました。\n",
            "\n",
            "モデルのコンパイルが完了しました。\n",
            "オプティマイザ: AdamW\n",
            "損失関数: SparseCategoricalCrossentropy\n",
            "メトリクス: ['accuracy']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 設定値 ---\n",
        "TRAIN_STEPS_FOR_TEST = 1  # 訓練ステップ数を1に設定\n",
        "VAL_STEPS_FOR_TEST = 1    # 検証ステップ数を1に設定\n",
        "EPOCHS_FOR_TEST = 1       # エポック数を1に設定\n",
        "# -------------\n",
        "\n",
        "print(\"モデルの訓練を開始します (最小ステップでのテスト実行)...\")\n",
        "print(f\"訓練データセットのステップ数 (テスト用): {TRAIN_STEPS_FOR_TEST}\")\n",
        "print(f\"検証データセットのステップ数 (テスト用): {VAL_STEPS_FOR_TEST}\")\n",
        "print(f\"エポック数 (テスト用): {EPOCHS_FOR_TEST}\")\n",
        "\n",
        "# model変数とtf_train_dataset, tf_val_dataset変数が\n",
        "# このコードの前に定義されていることを確認してください。\n",
        "# 例:\n",
        "# model = create_model() # あなたのモデル構築関数\n",
        "# tf_train_dataset = ... # あなたの訓練データセット\n",
        "# tf_val_dataset = ...   # あなたの検証データセット\n",
        "\n",
        "try:\n",
        "    # .take() を使ってデータセットから指定したステップ数だけを取り出す\n",
        "    # steps_per_epoch と validation_steps も指定して、1エポックあたりのステップ数を制限する\n",
        "    history = model.fit(\n",
        "        tf_train_dataset.take(TRAIN_STEPS_FOR_TEST), # 訓練データを1ステップ分だけ使用\n",
        "        validation_data=tf_val_dataset.take(VAL_STEPS_FOR_TEST), # 検証データを1ステップ分だけ使用\n",
        "        epochs=EPOCHS_FOR_TEST,                      # 1エポックだけ実行\n",
        "        steps_per_epoch=TRAIN_STEPS_FOR_TEST,        # 1エポックあたりの訓練ステップ数を1に指定\n",
        "        validation_steps=VAL_STEPS_FOR_TEST          # 1エポックあたりの検証ステップ数を1に指定\n",
        "    )\n",
        "    print(\"\\n最小ステップでのテスト実行 完了。\")\n",
        "\n",
        "    # 訓練履歴の確認 (オプション)\n",
        "    # ステップ数が1なので、あまり意味のある履歴にはなりませんが、形式は確認できます。\n",
        "    print(\"\\n訓練履歴 (最小ステップテスト):\")\n",
        "    print(history.history)\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\nモデルの訓練中(最小ステップテスト)にエラーが発生しました: {e}\")\n",
        "\n",
        "# --- この後のコード (例: model.evaluate, model.save など) ---\n",
        "# ... (学習はほぼ行われていないモデルで実行されることに注意) ...\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qg6W-9zZX9xq",
        "outputId": "0de0a377-988f-4291-931d-ab4f9ab1bff6"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "モデルの訓練を開始します (最小ステップでのテスト実行)...\n",
            "訓練データセットのステップ数 (テスト用): 1\n",
            "検証データセットのステップ数 (テスト用): 1\n",
            "エポック数 (テスト用): 1\n",
            "1/1 [==============================] - 89s 89s/step - loss: 0.7011 - accuracy: 0.3750 - val_loss: 0.7601 - val_accuracy: 0.0000e+00\n",
            "\n",
            "最小ステップでのテスト実行 完了。\n",
            "\n",
            "訓練履歴 (最小ステップテスト):\n",
            "{'loss': [0.7011189460754395], 'accuracy': [0.375], 'val_loss': [0.7601122856140137], 'val_accuracy': [0.0]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 設定値 ---\n",
        "EVAL_STEPS_FOR_TEST = 1 # 評価ステップ数を1に設定\n",
        "# -------------\n",
        "\n",
        "print(f\"\\nモデルの評価を開始します (最小ステップでのテスト実行)...\")\n",
        "print(f\"評価データセットのステップ数 (テスト用): {EVAL_STEPS_FOR_TEST}\")\n",
        "\n",
        "# model変数とtf_val_dataset変数が\n",
        "# このコードの前に定義されていることを確認してください。\n",
        "# 例:\n",
        "# model = ... (学習済み、またはfitのテスト実行後のモデル)\n",
        "# tf_val_dataset = ... (検証データセット)\n",
        "\n",
        "try:\n",
        "    # .take() を使ってデータセットから指定したステップ数だけを取り出す\n",
        "    # steps 引数を指定して、評価に使用するステップ数を制限する\n",
        "    results = model.evaluate(\n",
        "        tf_val_dataset.take(EVAL_STEPS_FOR_TEST), # 検証データを1ステップ分だけ使用\n",
        "        steps=EVAL_STEPS_FOR_TEST                 # 評価ステップ数を1に指定\n",
        "    )\n",
        "    print(\"\\n最小ステップでの評価実行 完了。\")\n",
        "\n",
        "    # 評価結果の表示 (形式確認)\n",
        "    # 損失と精度が表示されるはずです。値自体は学習度合いによります。\n",
        "    print(\"\\n評価結果 (最小ステップテスト):\")\n",
        "    print(f\"Loss: {results[0]}\")\n",
        "    print(f\"Accuracy: {results[1]}\") # model.compileで指定したメトリクス名に合わせる\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\nモデルの評価中(最小ステップテスト)にエラーが発生しました: {e}\")\n",
        "\n",
        "# --- この後のコード ---\n",
        "# ...\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6p4DuLU4bSw4",
        "outputId": "0c105526-1a61-4021-b951-d0dcfb54d058"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "モデルの評価を開始します (最小ステップでのテスト実行)...\n",
            "評価データセットのステップ数 (テスト用): 1\n",
            "1/1 [==============================] - 17s 17s/step - loss: 0.7601 - accuracy: 0.0000e+00\n",
            "\n",
            "最小ステップでの評価実行 完了。\n",
            "\n",
            "評価結果 (最小ステップテスト):\n",
            "Loss: 0.7601122856140137\n",
            "Accuracy: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "# --- 設定値 ---\n",
        "# tokenizer変数とmodel変数が定義されていることを確認\n",
        "# 例:\n",
        "# from transformers import AutoTokenizer\n",
        "# tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\") # または使用したモデルのTokenizer\n",
        "# model = ... (学習済み、またはfit/evaluateのテスト実行後のモデル)\n",
        "\n",
        "# テスト用の例文\n",
        "test_reviews = [\n",
        "    \"This movie was absolutely fantastic! Highly recommend.\",\n",
        "    \"What a waste of time, the plot was terrible.\",\n",
        "    \"It was okay, not great but not bad either.\"\n",
        "]\n",
        "\n",
        "print(\"\\n推論処理を開始します (テスト実行)...\")\n",
        "print(\"テスト用のレビュー:\")\n",
        "for review in test_reviews:\n",
        "    print(f\"- {review}\")\n",
        "\n",
        "try:\n",
        "    # 1. トークン化\n",
        "    #    Hugging FaceのTokenizerは通常、辞書形式でinput_idsなどを返す\n",
        "    #    TensorFlowモデルに入力するために、適切な形式に変換する\n",
        "    #    ここでは padding と truncation を有効にし、TensorFlowのテンソルを返すように指定\n",
        "    inputs = tokenizer(\n",
        "        test_reviews,\n",
        "        padding=True,\n",
        "        truncation=True,\n",
        "        return_tensors=\"tf\" # TensorFlow形式のテンソルで返す\n",
        "    )\n",
        "    print(\"\\nトークン化 完了。入力形式:\", inputs['input_ids'].shape) # 入力IDの形状を確認\n",
        "\n",
        "    # 2. モデルによる予測\n",
        "    #    モデルは通常、クラスごとの生のスコア (logits) を出力する\n",
        "    outputs = model(inputs) # モデルに入力を渡す\n",
        "\n",
        "    # Hugging FaceのTFモデルの場合、出力は TFSequenceClassifierOutput オブジェクトなどになることが多い\n",
        "    # その場合、logits 属性にアクセスする\n",
        "    # モデルの出力形式に合わせて調整が必要な場合があります\n",
        "    if hasattr(outputs, 'logits'):\n",
        "        logits = outputs.logits\n",
        "    elif isinstance(outputs, tf.Tensor): # 単純なTensorで返ってくる場合\n",
        "         logits = outputs\n",
        "    else:\n",
        "         # 不明な出力形式の場合、エラーを出すか、デバッグ情報を表示\n",
        "         print(\"警告: モデルの出力形式が想定外です。logitsの取得に失敗しました。\")\n",
        "         print(\"モデル出力:\", outputs)\n",
        "         logits = None # エラーを防ぐためNoneにしておく\n",
        "\n",
        "    if logits is not None:\n",
        "        print(\"\\nモデルによる予測 (logits) 完了。形状:\", logits.shape) # Logitsの形状を確認\n",
        "\n",
        "        # 3. 結果の解釈 (確率に変換し、クラスを決定)\n",
        "        #    Logitsをソフトマックス関数で確率に変換\n",
        "        probabilities = tf.nn.softmax(logits, axis=-1).numpy() # numpy配列に変換\n",
        "\n",
        "        # 各レビューに対する予測クラス (0: ネガティブ, 1: ポジティブ と仮定)\n",
        "        predicted_classes = np.argmax(probabilities, axis=-1)\n",
        "\n",
        "        print(\"\\n予測結果:\")\n",
        "        for i, review in enumerate(test_reviews):\n",
        "            pred_class = predicted_classes[i]\n",
        "            pred_label = \"ポジティブ\" if pred_class == 1 else \"ネガティブ\" # ラベルのマッピングが必要\n",
        "            confidence = probabilities[i][pred_class] # 予測クラスの確率\n",
        "            print(f\"- レビュー: \\\"{review[:50]}...\\\"\") # 長いレビューは省略\n",
        "            print(f\"  予測: {pred_label} (確率: {confidence:.4f})\")\n",
        "            # print(f\"  (クラス0確率: {probabilities[i][0]:.4f}, クラス1確率: {probabilities[i][1]:.4f})\") # 詳細表示用\n",
        "\n",
        "    print(\"\\n推論処理のテスト実行 完了。\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\n推論処理中(テスト実行)にエラーが発生しました: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc() # 詳細なエラー情報を表示\n",
        "\n",
        "\n",
        "# --- この後のコード ---\n",
        "# ...\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MMJM8ikqbkQ7",
        "outputId": "11cc7209-0a2c-4cc4-a7d0-69118b1fc449"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "推論処理を開始します (テスト実行)...\n",
            "テスト用のレビュー:\n",
            "- This movie was absolutely fantastic! Highly recommend.\n",
            "- What a waste of time, the plot was terrible.\n",
            "- It was okay, not great but not bad either.\n",
            "\n",
            "トークン化 完了。入力形式: (3, 13)\n",
            "\n",
            "モデルによる予測 (logits) 完了。形状: (3, 2)\n",
            "\n",
            "予測結果:\n",
            "- レビュー: \"This movie was absolutely fantastic! Highly recomm...\"\n",
            "  予測: ポジティブ (確率: 0.5018)\n",
            "- レビュー: \"What a waste of time, the plot was terrible....\"\n",
            "  予測: ポジティブ (確率: 0.5007)\n",
            "- レビュー: \"It was okay, not great but not bad either....\"\n",
            "  予測: ポジティブ (確率: 0.5011)\n",
            "\n",
            "推論処理のテスト実行 完了。\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "JzIFxA5sFIJq"
      }
    }
  ]
}